{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fd7e04-75f7-4391-90cd-f64d80ceccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils\n",
    "\n",
    "#自作ライブラリ\n",
    "import tokenizer as tk\n",
    "import mini_transformer as mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a487ae-a431-472b-992d-05f033df8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a60e373-70a1-4e64-ae34-0a5b38db8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac0605f-b71a-480c-b184-b7b7fe029e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = tk.Tokenizer()\n",
    "tokenizer.set_list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51fa898-8131-4809-9624-6bb87966cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tokenizer.train_test_split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affdb51f-4f08-42f6-af09-58327417d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini.GPTLanguageModel(tokenizer.chars).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152eb069-4846-4c98-a03d-8fd2ad9927a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = tokenizer.get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2da4f08-89e0-453f-9137-e33ed8ee9e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n",
      "1003597 (64,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_batch(split)\n\u001b[1;32m      9\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[0;32m---> 10\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = tokenizer.get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c474ba-79ca-40c7-8fea-8896287c8ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ySoh,' ce ttye oeatRe thraD Iflg:a,,sgnptOeamhoahereo  nene s, i\n",
      "ameN i oudhdr  es rU frrNuorH,sr,yrHonisfsdh,m eni  w Lg\n",
      "qthidtthdeh andha'h as th  .Nt m\n",
      "He. anpg\n",
      "Iedoauim u's sp,  ss\n",
      "lndtnwhatnpthinimwo adrfutthe  t\n",
      " Cl heyl KhcS scon\n",
      "Ktye a lhtsohrieith r,cnGn aeuer sD.ihein haveI a\n",
      "Iatyo w\n",
      ":enh  numgt iOithaDnthdrN ev?N\n",
      " iyTe  sIf nl\n",
      "he f eaIcRoe head:\n",
      "hhwEr.c\n",
      "AiAn:vsd na\n",
      "d haemetc  w\n",
      "D,se dhEone,\n",
      "  ay:\n",
      "ena  oAtFRuora\n",
      "oe e :  a ;saIPte CInerlnthhauoisre,inde nRo a: n oo all,wovinc ddlGinoter\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(tokenizer.decoder(model.generate(context, max_new_token=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170894dc-4af8-4e33-8c66-3711ba08140a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
